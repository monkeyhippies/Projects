#Machine Learning Project

First, we import the data. We also partition our data into a train and test set and set our last tests as our validation set.  We set our seed for the whole analysis and open the necessary libraries.

```{r ,cache=TRUE}
setInternet2(TRUE)
data=read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
training=data;
validation=read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
library(caret)
library(klaR)
library(rpart)

inTrain=createDataPartition(y=training$classe,p=.75,list=FALSE)
train=training[inTrain,]
test=training[-inTrain,]
set.seed(25)

```

##Exploratory Analysis:

Here, we see that our different classes are mostly evenly distributed except with more in class A.

Also, we do a density plot and a plot of total_accel_dumbell versus roll_belt with classe indicated by color.  We see that it is hard to distinguish between classes based on total_accel_dumbell, but roll_belt variable seems to distinguish class E somewhat from the rest of the variables.

```{r, cache=TRUE}
table(train$classe)
qplot(total_accel_dumbbell,colour=classe,data=train,geom="density")
qplot(total_accel_dumbbell,roll_belt,colour=classe,data=train)
```

##Preprocess:

We preprocess a little by first deleting any variables from our data that probably won't be useful.  We start by deleting any columns with too many NA's.  Then we delete the names column because we shouldn't be predicting based on what person is doing the activity. Next, we look at our variables and notice that the variables that relate to the forearm, arm, dumbbell, or belt are probably the most relevant while other variables like time and date probably aren't.  We only take the variables with the aforementioned words. Last, we take out any variables that have too little variance to further trim down.

```{r, cache=TRUE}
sums=sapply(train,function(x){sum(is.na(x))})  ##delete variables with too many NAs
train=train[,sums<10]

train=train[,!colnames(train) %in% "user_name"] ##delete names column
index=grep("_forearm|_arm|_dumbbell|_belt|classe",names(train)) ## only take columns with following words
train=train[,index]

nsv=nearZeroVar(train,saveMetrics=TRUE) ##take out variables with too little variance
train=train[,!nsv$nzv]
```

###Select predictors for model

We do a simple binary tree against our outcome "classe", using only 1 predictor at at time to see if some predictors could be more useful than others.  We split our train data with a k-split.  Then we train the data on each train split and test on each test split.  We then average our accuracies to get a better cross-validation estimate of the accuracy. W notice that each variable has around the same accuracy, although _belt variables score the highest, around 40%.  At the end, we delete the unnecessary columns from our test and validation test, too.

```{r, cache=TRUE}
set.seed(25)
variable_scores=vector("list",ncol(train)-1)
names(variable_scores)=names(train[,!names(train) %in% "classe"])
for (name in names(train[,!names(train) %in% "classe"])){
        folds=createFolds(y=train$classe,k=5,returnTrain=TRUE)
        sums=0
        for(i in 1:2){
        data=train[,c(name[[1]],"classe")]
        obj=train(data$classe~.,data=data,method="rpart")
        data2=train[-folds[[1]],]
        pred=predict(obj,data2)
        accuracy=sum(pred==train[-folds[[1]],"classe"])/nrow(train[-folds[[1]],])
        sums=sums+accuracy
        }
        variable_scores[name[[1]]]=sums/2
        
}
variable_scores=unlist(variable_scores)
variable_scores=sort(variable_scores,decreasing=TRUE) ##We note there are no variables that particularly stand out to eliminate, although _belt variables seem to be the most important


names=names(train) ##here are the names of columns that we keep; we delete them from our test data too
test=test[,names(test) %in% names]
validation=validation[,names(validation) %in% c(names,"problem_id")]
```

Next, we run PCA on each group of variables (grouped by "arm", "forearm", "belt", and "dumbell" in column names).  We group by variables mainly to keep our new predictors a little more interpretable and to guess that these variables are more related to each other and should be treated as such to reduce overfitting or noninterpretable results.  However, doing PCA by groups could also decrease our accuracy. At the end, we combine the PCA predictors together.  We also create PCA  sets for our test and validation.  With "PCA", we capture 95% variance from each group.  In total, we end with 32 predictors from an initial 159.

```{r, cache=TRUE}
##Preprocessing: PCA to reduce number of variables further, dividing by each body part, because we assume each body part is correlated

##_arm variables PCA
preProc_arm=preProcess(train[,grep("_arm",colnames(train))],method="pca")
pred_arm=predict(preProc_arm,train[,grep("_arm",colnames(train))])
pred_arm_test=predict(preProc_arm,test[,grep("_arm",colnames(test))])
pred_arm_validation=predict(preProc_arm,validation[,grep("_arm",colnames(validation))])

##_forearm variables PCA
preProc_forearm=preProcess(train[,grep("_forearm",colnames(train))],method="pca")
pred_forearm=predict(preProc_forearm,train[,grep("_forearm",colnames(train))])
pred_forearm_test=predict(preProc_forearm,test[,grep("_forearm",colnames(test))])
pred_forearm_validation=predict(preProc_forearm,validation[,grep("_forearm",colnames(validation))])

##_dumbell variables PCA
preProc_dumbbell=preProcess(train[,grep("_dumbbell",colnames(train))],method="pca")
pred_dumbbell=predict(preProc_dumbbell,train[,grep("_dumbbell",colnames(train))])
pred_dumbbell_test=predict(preProc_dumbbell,test[,grep("_dumbbell",colnames(test))])
pred_dumbbell_validation=predict(preProc_dumbbell,validation[,grep("_dumbbell",colnames(validation))])

##_belt variables PCA
preProc_belt=preProcess(train[,grep("_belt",colnames(train))],method="pca")
pred_belt=predict(preProc_belt,train[,grep("_belt",colnames(train))])
pred_belt_test=predict(preProc_belt,test[,grep("_belt",colnames(test))])
pred_belt_validation=predict(preProc_belt,validation[,grep("_belt",colnames(validation))])

##combine PCAs
trainPCA=data.frame(pred_arm,pred_forearm,pred_dumbbell,pred_belt,train$classe) ##ended up with 32 variables
testPCA=data.frame(pred_arm_test,pred_forearm_test,pred_dumbbell_test,pred_belt_test,test$classe)
validationPCA=data.frame(pred_arm_validation,pred_forearm_validation,pred_dumbbell_validation,pred_belt_validation,validation$problem_id)
names(testPCA)=names(trainPCA)
names(validationPCA)=names(trainPCA)
```

##Make Prediction Models:
We use rpart classifiction tree, linear discriminant analysis (lda), and naive bayes (nb) tests.  Then we combine all predictors (majority vote) (normally,I would ksplit the data, run each test on each k split, take average accuracies, to see if I want to use them.  But since it would take forever, I will assume these are the 3 models I want.  Also, I use these because they are the only ones that work in a reasonable amoutn of time).  We first split (k=5) the data (we only use 200 and 100 data points for each split for train and test respectively because my computer is too slow) and use cross validation to measure the accuracy of each test individually.  The most accurate test will be the decider when voting produces no majority. In the end, we see that naive bayes works best with lda in a vey close second.  We set naive bayes as our deciding voter when there is a tie.

```{r, cache=TRUE}
accuracy1=0
accuracy2=0
accuracy3=0
for (i in 1:5){
index=sample(1:nrow(trainPCA),300)
trainsample=trainPCA[index[1:200],]
testsample=trainPCA[index[201:300],]

obj1=train(train.classe~.,data=trainsample,method="rpart")
obj2=train(train.classe~.,data=trainsample,method="nb")
obj3=train(train.classe~.,data=trainsample,method="lda")

pred1=predict(obj1,testsample)
pred2=predict(obj2,testsample)
pred3=predict(obj3,testsample)

accuracy1=accuracy1+sum(pred1==testsample$train.classe)/nrow(testsample)
accuracy2=accuracy2+sum(pred2==testsample$train.classe)/nrow(testsample)
accuracy3=accuracy3+sum(pred3==testsample$train.classe)/nrow(testsample)
}

accuracy1=accuracy1/5
accuracy1
accuracy2=accuracy2/5
accuracy2
accuracy3=accuracy3/5 ## We note that test 2 is best, so we will use it in case of no majority for majority vote of data
accuracy3
```

##Predicted Accuracy:

We predict our accuracy of our model by using our test data.  We k-split (k=5) our data.  Then use cross validation to average the accuracies together.  Our overall accuracy is estimated at 53%, which is pretty bad, but it's because I couldn't use all of the data to train because of a slow computer.

```{r, cache=TRUE}
##predict on test set
folds=createFolds(y=test$classe,k=5,returnTrain=FALSE)
accuracy1=0
accuracy2=0
accuracy3=0
total_accuracy=0
for (fold in folds){
        dataset=testPCA[fold,]
        dataset=testPCA
        pred1=predict(obj1,dataset)
        pred2=predict(obj2,dataset)
        pred3=predict(obj3,dataset)
        accuracy1=accuracy1+sum(pred1==dataset$train.classe)/nrow(dataset)
        accuracy2=accuracy2+sum(pred2==dataset$train.classe)/nrow(dataset)
        accuracy3=accuracy3+sum(pred3==dataset$train.classe)/nrow(dataset)
        pred13=pred1==pred3
        total_pred=ifelse(pred13,as.character(pred1),as.character(pred2))
        total_accuracy=total_accuracy+sum(total_pred==as.character(dataset$train.classe))/nrow(dataset)
}
accuracy1=accuracy1/5
accuracy2=accuracy2/5
accuracy3=accuracy3/5
total_accuracy=total_accuracy/5

total_accuracy ##this is predicted accuracy using cross-validation of test set

```

#Run model on validation:
I test on our validation.  Our predictions are ouputted below!
```{r}
dataset=validationPCA
pred1=predict(obj1,dataset)
pred2=predict(obj2,dataset)
pred3=predict(obj3,dataset)
pred12=pred1==pred2
total_pred=ifelse(pred12,as.character(pred1),as.character(pred3))
total_pred


```
