#Regression Project

##Exploratory Analysis:
Remember that 0=Automatic and 1=Manual

First, we did some simple summaries of the data.  As you can see, there are 16 manual and 16 automatic cars

```{r}
data(mtcars)
data=data.frame(mtcars)
table(data$am)
```


At first glance, it seems that manual has better mileage than automatic.
```{r}
mpg_manual=subset(data,data$am==1)
mpg_automatic=subset(data,data$am==0)
summary(mpg_manual$mpg)
summary(mpg_automatic$mpg)
```

However, it seems that automatic cars are generally heavier,have more cylinders, and have more horsepower, and more displacement, which can all be factors in causing worse mileage. (Look at pairs function output in appendix) (Also, histogram in Appendix)
```{r}
sapply(mpg_manual,mean)
sapply(mpg_automatic,mean)
```


##Here's some models

First, look just at mpg versus manual/automatic.This shows that ignoring all other variables, 95% confidence interval shows manual has better mileage.
```{r}
fits=lm(data$mpg~factor(data$am))
        fits_coeff=summary(fits)$coefficients
        fits_coeff[2,1]
        qt(.975,32-4)*fits_coeff[2,2]*c(-1,1)+fits_coeff[2,1]
```

We do a model comparing mpg and weight since weight is known to affect mpg a lot (look at pairs() diagram for confirmation of this) (Plots of fit0 are in Appendix)
```{r}
fit0=lm(data$mpg~data$wt)
```

Here, we add manual/automatic variable as an interaction with weight.  We also calculate the confidence interval. The model shows that within 95% confidence interval, automatic has better mpg for high weight and worse for lighter weight (around a little less than 3000 pounds is where intersect is)
```{r}
fit=lm(data$mpg~data$wt+data$am*data$wt)
plot(data$wt,data$mpg,col=(data$am==0)*1+1)
abline(c(fit$coefficients[1],fit$coefficients[2]),col=2)
abline(c(fit$coefficients[1]+fit$coefficients[3],fit$coefficients[2]+fit$coefficients[4]),col=1)
title("Red=Automatic; Black=Manual")
        ###Calculate confidence interval of fit
                fit_coeff=summary(fit)$coefficients
                qt(.975,32-4)*fit_coeff[4,2]*c(-1,1)+fit_coeff[4,1] ##

```

We compare several models below.  Our analysis shows that fitting data; It shows that we can account for weight, manual/automatic, and displacement, but other variables seem unnecessary. Note that fit2 doesn't change the coefficient values for data$am too much.

```{r}
fit2=lm(data$mpg~data$wt+data$am*data$wt+data$disp)
fit3=lm(data$mpg~data$wt+data$am*data$wt+data$disp+data$hp)
anova(fit0,fit,fit2,fit3)  
```

###Conclusion: 

Overall, it seems that manual cars have better mileage at low weights while automatic cars have better mileage at higher weights.  This is shown from our model, fit.  However, we can't really make a strong conclusion about this becuase the automatic cars are almost all heavier than manual cars in general. Although this is clearly shown in the plots, we do a t-test in appendix, comparing weights of manual to automatic cars to show that we reject null hypothesis with two-sided 95% confidence p-value. There's no intersection of data.  Note that analysis of residuals produces no significant problems to note.  


##APPENDIX
```{r}
###From graphs, it seems weight, horsepower, number of cylinders, displacement all seem to negatively affect mpg.  Knowing this, it's unclear how automatic and manual settings affect mpg
pairs(data) 
```

Here's a histogram to give us a better look at the mpg of automatic and manual cars.  Manual cars seem to have more MPG.
```{r}
library(ggplot2)
data$am=factor(data$am)
qplot(mpg,data=data,color=am,binwidth=2,fill=am)
```

```{r}
###plot of model fit0 
plot(data$wt,data$mpg,col=(data$am==0)*1+1)
abline(c(fit0$coefficients[1],fit0$coefficients[2]),col=4)
title("Red=Automatic; Black=Manual")
```


###Residuals and Diagnostics:

We calculate some residuals. The plot shows residuals are good for model fit, no pattern, and talking regression of resids shows no slope to residuals.
```{r}
plot(data$wt,resid(fit))
lm(data$wt~resid(fit))##
```

Fits shows a slight downward slope to resids, which indicates this model might not be that good.  This is shown below.
```{r}
plot(data$wt,resid(fits))
lm(data$wt~resid(fits)) 
```

Doing a dfbetas, we not that the max in dfbetas for our interaction coefficient(weight&am) is relatively small ~.4, which is less than 10% of coefficient's value.
```{r}
dfbetas=dfbetas(fit)
max(dfbetas[,4]) ##
```

```{r}
t.test(mpg_manual,mpg_automatic)
```